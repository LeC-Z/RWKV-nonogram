{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4NLMbaWOy+spHvkZiorAk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-Nkhz0iQskc","executionInfo":{"status":"ok","timestamp":1727218832861,"user_tz":420,"elapsed":12557,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}},"outputId":"7776e811-aedd-4346-fb2f-86a37872ec97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RWKV-nonogram'...\n","remote: Enumerating objects: 23, done.\u001b[K\n","remote: Counting objects: 100% (23/23), done.\u001b[K\n","remote: Compressing objects: 100% (22/22), done.\u001b[K\n","remote: Total 23 (delta 8), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (23/23), 38.79 MiB | 8.87 MiB/s, done.\n","Resolving deltas: 100% (8/8), done.\n","Collecting rwkv\n","  Downloading rwkv-0.8.26-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from rwkv) (0.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->rwkv) (0.24.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2024.8.30)\n","Downloading rwkv-0.8.26-py3-none-any.whl (406 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.2/406.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rwkv\n","Successfully installed rwkv-0.8.26\n"]}],"source":["!git clone https://github.com/LeC-Z/RWKV-nonogram.git\n","!pip install rwkv\n","%cd RWKV-nonogram/\n","!mv ./rwkv_vocab_nonogram.txt rwkv_vocab_v20230424.txt\n","!cp ./rwkv_vocab_v20230424.txt /usr/local/lib/python3.10/dist-packages/rwkv/"]},{"cell_type":"code","source":["from rwkv.model import RWKV\n","from rwkv.utils import PIPELINE, PIPELINE_ARGS\n","\n","# download models: https://huggingface.co/BlinkDL\n","model = RWKV(model='./rwkv-11.pth', strategy='cpu fp32')\n","pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV\n","# use pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\") for rwkv \"world\" models\n","\n","def my_print(s):\n","    print(s, end='', flush=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzwLFG0GSZZl","executionInfo":{"status":"ok","timestamp":1727219403379,"user_tz":420,"elapsed":9790,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}},"outputId":"fabb9dc7-b95c-463e-a5af-bce8a1c193f7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n","\n","Loading ./rwkv-11.pth ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/rwkv/model.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.w = torch.load(args.MODEL_NAME, map_location='cpu') # load model to CPU first\n"]},{"output_type":"stream","name":"stdout","text":["Model detected: v6.0\n","Strategy: (total 12+1=13 layers)\n","* cpu [float32, float32], store 13 layers\n","0-cpu-float32-float32 1-cpu-float32-float32 2-cpu-float32-float32 3-cpu-float32-float32 4-cpu-float32-float32 5-cpu-float32-float32 6-cpu-float32-float32 7-cpu-float32-float32 8-cpu-float32-float32 9-cpu-float32-float32 10-cpu-float32-float32 11-cpu-float32-float32 12-cpu-float32-float32 \n","emb.weight                        f32      cpu     17   384       \n","blocks.0.ln1.weight               f32      cpu    384             \n","blocks.0.ln1.bias                 f32      cpu    384             \n","blocks.0.ln2.weight               f32      cpu    384             \n","blocks.0.ln2.bias                 f32      cpu    384             \n","blocks.0.att.time_maa_x           f32      cpu    384             \n","blocks.0.att.time_maa_w           f32      cpu    384             \n","blocks.0.att.time_maa_k           f32      cpu    384             \n","blocks.0.att.time_maa_v           f32      cpu    384             \n","blocks.0.att.time_maa_r           f32      cpu    384             \n","blocks.0.att.time_maa_g           f32      cpu    384             \n","blocks.0.att.time_maa_w1          f32      cpu    384   160       \n","blocks.0.att.time_maa_w2          f32      cpu      5    32   384 \n","blocks.0.att.time_decay           f32      cpu      6    64       \n","blocks.0.att.time_decay_w1        f32      cpu    384    64       \n","blocks.0.att.time_decay_w2        f32      cpu     64   384       \n","blocks.0.att.time_first           f32      cpu      6    64       \n","blocks.0.att.receptance.weight    f32      cpu    384   384       \n","blocks.0.att.key.weight           f32      cpu    384   384       \n","blocks.0.att.value.weight         f32      cpu    384   384       \n","blocks.0.att.output.weight        f32      cpu    384   384       \n","blocks.0.att.gate.weight          f32      cpu    384   384       \n","blocks.0.att.ln_x.weight          f32      cpu    384             \n","blocks.0.att.ln_x.bias            f32      cpu    384             \n","blocks.0.ffn.time_maa_k           f32      cpu    384             \n","blocks.0.ffn.time_maa_r           f32      cpu    384             \n","blocks.0.ffn.key.weight           f32      cpu    384  1344       \n","blocks.0.ffn.receptance.weight    f32      cpu    384   384       \n","blocks.0.ffn.value.weight         f32      cpu   1344   384       \n","........................................................................................................................................................................................................................................................................................\n","blocks.11.ln1.weight              f32      cpu    384             \n","blocks.11.ln1.bias                f32      cpu    384             \n","blocks.11.ln2.weight              f32      cpu    384             \n","blocks.11.ln2.bias                f32      cpu    384             \n","blocks.11.att.time_maa_x          f32      cpu    384             \n","blocks.11.att.time_maa_w          f32      cpu    384             \n","blocks.11.att.time_maa_k          f32      cpu    384             \n","blocks.11.att.time_maa_v          f32      cpu    384             \n","blocks.11.att.time_maa_r          f32      cpu    384             \n","blocks.11.att.time_maa_g          f32      cpu    384             \n","blocks.11.att.time_maa_w1         f32      cpu    384   160       \n","blocks.11.att.time_maa_w2         f32      cpu      5    32   384 \n","blocks.11.att.time_decay          f32      cpu      6    64       \n","blocks.11.att.time_decay_w1       f32      cpu    384    64       \n","blocks.11.att.time_decay_w2       f32      cpu     64   384       \n","blocks.11.att.time_first          f32      cpu      6    64       \n","blocks.11.att.receptance.weight   f32      cpu    384   384       \n","blocks.11.att.key.weight          f32      cpu    384   384       \n","blocks.11.att.value.weight        f32      cpu    384   384       \n","blocks.11.att.output.weight       f32      cpu    384   384       \n","blocks.11.att.gate.weight         f32      cpu    384   384       \n","blocks.11.att.ln_x.weight         f32      cpu    384             \n","blocks.11.att.ln_x.bias           f32      cpu    384             \n","blocks.11.ffn.time_maa_k          f32      cpu    384             \n","blocks.11.ffn.time_maa_r          f32      cpu    384             \n","blocks.11.ffn.key.weight          f32      cpu    384  1344       \n","blocks.11.ffn.receptance.weight   f32      cpu    384   384       \n","blocks.11.ffn.value.weight        f32      cpu   1344   384       \n","ln_out.weight                     f32      cpu    384             \n","ln_out.bias                       f32      cpu    384             \n","head.weight                       f32      cpu    384    17       \n"]}]},{"cell_type":"markdown","source":["using the similar format as follow **exactly**:\n","> \"[[3], [2], [4], [2], [2]]\\n[[3], [4], [3], [1, 1], [1]]\\n\\n\"\n","\n","[3], [2], [4], [2], [2]] is the row prompts.\n","\n","[[3], [4], [3], [1, 1], [1]] is the col prompts.\n","\n","\n","`']]'` is the symbols of end."],"metadata":{"id":"MH9L2M1ITIrX"}},{"cell_type":"code","source":["args = PIPELINE_ARGS(temperature = 0.8, top_p = 0.2, top_k = 100, # top_k = 0 then ignore\n","                     alpha_frequency = 0,\n","                     alpha_presence = 0,\n","                     alpha_decay = 0.996, # gradually decay the penalty\n","                     token_ban = [0], # ban the generation of some tokens\n","                     token_stop = [''], # stop generation whenever you see any token here\n","                     chunk_len = 1024) # split input into chunks to save VRAM (shorter -> slower)\n","\n","\n","ctx = \"[[3], [2], [4], [2], [2]]\\n[[3], [4], [3], [1, 1], [1]]\\n\\n\"\n","pipeline.generate(ctx, token_count=3000, args=args, callback=my_print)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bN84mkMGSbYv","executionInfo":{"status":"ok","timestamp":1727219627403,"user_tz":420,"elapsed":101786,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}},"outputId":"f3141af3-f818-4c75-f11f-dd2c204c6d14"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[3] [4] [3] [1, 1] [1] \n","00000:[3]\n","00000:[2]\n","00000:[4]\n","00000:[2]\n","00000:[2]\n","R1:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","00000:[4]\n","00000:[2]\n","00000:[2]\n","R2:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","00000:[4]\n","00000:[2]\n","00000:[2]\n","R3:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","0###0:[4]\n","00000:[2]\n","00000:[2]\n","R4:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","0###0:[4]\n","00000:[2]\n","00000:[2]\n","R5:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","0###0:[4]\n","00000:[2]\n","00000:[2]\n","C1:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","00000:[2]\n","####0:[4]\n","00000:[2]\n","00000:[2]\n","C2:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","0#000:[2]\n","####0:[4]\n","0#000:[2]\n","00000:[2]\n","C3:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","0##00:[2]\n","####0:[4]\n","0#X00:[2]\n","00X00:[2]\n","C4:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","0##X0:[2]\n","####0:[4]\n","0#XX0:[2]\n","00X00:[2]\n","C5:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","0##X0:[2]\n","####0:[4]\n","0#XX0:[2]\n","00X00:[2]\n","R1:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","0##X0:[2]\n","####0:[4]\n","0#XX0:[2]\n","00X00:[2]\n","R2:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","X##XX:[2]\n","####0:[4]\n","0#XX0:[2]\n","00X00:[2]\n","R3:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","X##XX:[2]\n","####X:[4]\n","0#XX0:[2]\n","00X00:[2]\n","R4:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","00X00:[2]\n","R5:\n","[3] [4] [3] [1, 1] [1] \n","00#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","00X00:[2]\n","C1:\n","[3] [4] [3] [1, 1] [1] \n","X0#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","C2:\n","[3] [4] [3] [1, 1] [1] \n","X0#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","C3:\n","[3] [4] [3] [1, 1] [1] \n","X0#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","C4:\n","[3] [4] [3] [1, 1] [1] \n","X0#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","C5:\n","[3] [4] [3] [1, 1] [1] \n","X0#00:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","R1:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","R2:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","R3:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","R4:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","#0X00:[2]\n","R5:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","C1:\n","[3] [4] [3] [1, 1] [1] \n","X0##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","C2:\n","[3] [4] [3] [1, 1] [1] \n","XX##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","C3:\n","[3] [4] [3] [1, 1] [1] \n","XX##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","C4:\n","[3] [4] [3] [1, 1] [1] \n","XX##0:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","C5:\n","[3] [4] [3] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","]]\n","##X#X:[2, 1]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","R4:\n","[3] [4] [3] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","]]\n","]]\n","]]\n","[3] [4] [3] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","####X:[4]\n","##XXX:[2]\n","##XXX:[2]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","[4] [4] [2] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","##X#X:[2, 1]\n","##XXX:[2]\n","0##XX:[3]\n","]]\n","R4:\n","[4] [4] [2] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","##X#X:[2, 1]\n","##XXX:[2]\n","0##XX:[3]\n","R5:\n","[4] [4] [2] [1, 1] [1] \n","XX###:[3]\n","X##XX:[2]\n","##X#X:[2, 1]\n","##XXX:[2]\n","###XX:[3]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","]]\n","[4] [4] [2] [1, 1] [1] \n","XX###:[3]\n","##XXX:[2]\n","##X#X:[2, 1]\n","##XXX:[2]\n","###XX:["]},{"output_type":"execute_result","data":{"text/plain":["'[3] [4] [3] [1, 1] [1] \\n00000:[3]\\n00000:[2]\\n00000:[4]\\n00000:[2]\\n00000:[2]\\nR1:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n00000:[4]\\n00000:[2]\\n00000:[2]\\nR2:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n00000:[4]\\n00000:[2]\\n00000:[2]\\nR3:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n0###0:[4]\\n00000:[2]\\n00000:[2]\\nR4:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n0###0:[4]\\n00000:[2]\\n00000:[2]\\nR5:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n0###0:[4]\\n00000:[2]\\n00000:[2]\\nC1:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n00000:[2]\\n####0:[4]\\n00000:[2]\\n00000:[2]\\nC2:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n0#000:[2]\\n####0:[4]\\n0#000:[2]\\n00000:[2]\\nC3:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n0##00:[2]\\n####0:[4]\\n0#X00:[2]\\n00X00:[2]\\nC4:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n0##X0:[2]\\n####0:[4]\\n0#XX0:[2]\\n00X00:[2]\\nC5:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n0##X0:[2]\\n####0:[4]\\n0#XX0:[2]\\n00X00:[2]\\nR1:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\n0##X0:[2]\\n####0:[4]\\n0#XX0:[2]\\n00X00:[2]\\nR2:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\nX##XX:[2]\\n####0:[4]\\n0#XX0:[2]\\n00X00:[2]\\nR3:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\nX##XX:[2]\\n####X:[4]\\n0#XX0:[2]\\n00X00:[2]\\nR4:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n00X00:[2]\\nR5:\\n[3] [4] [3] [1, 1] [1] \\n00#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n00X00:[2]\\nC1:\\n[3] [4] [3] [1, 1] [1] \\nX0#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nC2:\\n[3] [4] [3] [1, 1] [1] \\nX0#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nC3:\\n[3] [4] [3] [1, 1] [1] \\nX0#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nC4:\\n[3] [4] [3] [1, 1] [1] \\nX0#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nC5:\\n[3] [4] [3] [1, 1] [1] \\nX0#00:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nR1:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nR2:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nR3:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nR4:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n#0X00:[2]\\nR5:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\nC1:\\n[3] [4] [3] [1, 1] [1] \\nX0##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\nC2:\\n[3] [4] [3] [1, 1] [1] \\nXX##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\nC3:\\n[3] [4] [3] [1, 1] [1] \\nXX##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\nC4:\\n[3] [4] [3] [1, 1] [1] \\nXX##0:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\nC5:\\n[3] [4] [3] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\n]]\\n##X#X:[2, 1]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\nR4:\\n[3] [4] [3] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\n]]\\n]]\\n]]\\n[3] [4] [3] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n####X:[4]\\n##XXX:[2]\\n##XXX:[2]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n[4] [4] [2] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n##X#X:[2, 1]\\n##XXX:[2]\\n0##XX:[3]\\n]]\\nR4:\\n[4] [4] [2] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n##X#X:[2, 1]\\n##XXX:[2]\\n0##XX:[3]\\nR5:\\n[4] [4] [2] [1, 1] [1] \\nXX###:[3]\\nX##XX:[2]\\n##X#X:[2, 1]\\n##XXX:[2]\\n###XX:[3]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n]]\\n[4] [4] [2] [1, 1] [1] \\nXX###:[3]\\n##XXX:[2]\\n##X#X:[2, 1]\\n##XXX:[2]\\n###XX:['"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":[],"metadata":{"id":"pz9FuMlSScAD","executionInfo":{"status":"ok","timestamp":1727219342994,"user_tz":420,"elapsed":199,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NGGfVnV_Q7KI","executionInfo":{"status":"ok","timestamp":1727219373251,"user_tz":420,"elapsed":3,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-B_oqzURB9F","executionInfo":{"status":"ok","timestamp":1727218950408,"user_tz":420,"elapsed":202,"user":{"displayName":"Lecheng Zhang","userId":"07091290722177708490"}},"outputId":"aa66cf60-1140-49cd-feca-dc5499d74a46"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["LICENSE  README.md  rwkv-11.pth  rwkv.gif  rwkv_vocab_nonogram.txt  visualize.gif\n"]}]}]}